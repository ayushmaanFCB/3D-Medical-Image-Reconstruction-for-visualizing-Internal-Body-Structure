{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPmUAQ9dPCUBdHHl/j+CGWa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushmaanFCB/3D-Medical-Image-Reconstruction-for-visualizing-Internal-Body-Structure/blob/main/UNet_3D_Vision_Transformer_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tcia_utils pydicom monai pytorch_msssim"
      ],
      "metadata": {
        "id": "ZigBHHYXL5CH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from tcia_utils import nbia"
      ],
      "metadata": {
        "id": "9lX6rUjZLwxR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get list of available collections as JSON\n",
        "print(nbia.getCollections())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wa-VX3ePMDU1",
        "outputId": "6b83a717-89d8-4df6-f6c5-cb328fe55c32"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'Collection': '4D-Lung'}, {'Collection': 'ACRIN-6698'}, {'Collection': 'ACRIN-Contralateral-Breast-MR'}, {'Collection': 'ACRIN-FLT-Breast'}, {'Collection': 'ACRIN-NSCLC-FDG-PET'}, {'Collection': 'Adrenal-ACC-Ki67-Seg'}, {'Collection': 'Advanced-MRI-Breast-Lesions'}, {'Collection': 'Anti-PD-1_Lung'}, {'Collection': 'B-mode-and-CEUS-Liver'}, {'Collection': 'BREAST-DIAGNOSIS'}, {'Collection': 'Breast-Cancer-Screening-DBT'}, {'Collection': 'Breast-MRI-NACT-Pilot'}, {'Collection': 'C4KC-KiTS'}, {'Collection': 'CBIS-DDSM'}, {'Collection': 'CC-Radiomics-Phantom'}, {'Collection': 'CC-Radiomics-Phantom-2'}, {'Collection': 'CC-Radiomics-Phantom-3'}, {'Collection': 'CC-Tumor-Heterogeneity'}, {'Collection': 'CMB-AML'}, {'Collection': 'CMB-CRC'}, {'Collection': 'CMB-GEC'}, {'Collection': 'CMB-LCA'}, {'Collection': 'CMB-MEL'}, {'Collection': 'CMB-MML'}, {'Collection': 'CMB-PCA'}, {'Collection': 'CMMD'}, {'Collection': 'COVID-19-AR'}, {'Collection': 'COVID-19-NY-SBU'}, {'Collection': 'CPTAC-CCRCC'}, {'Collection': 'CPTAC-CM'}, {'Collection': 'CPTAC-LSCC'}, {'Collection': 'CPTAC-LUAD'}, {'Collection': 'CPTAC-PDA'}, {'Collection': 'CPTAC-SAR'}, {'Collection': 'CPTAC-UCEC'}, {'Collection': 'CT COLONOGRAPHY'}, {'Collection': 'CT Lymph Nodes'}, {'Collection': 'CT-Phantom4Radiomics'}, {'Collection': 'CT-vs-PET-Ventilation-Imaging'}, {'Collection': 'CTpred-Sunitinib-panNET'}, {'Collection': 'Colorectal-Liver-Metastases'}, {'Collection': 'DRO-Toolkit'}, {'Collection': 'Duke-Breast-Cancer-MRI'}, {'Collection': 'EA1141'}, {'Collection': 'GBM-DSC-MRI-DRO'}, {'Collection': 'HCC-TACE-Seg'}, {'Collection': 'ICDC-Glioma'}, {'Collection': 'ISPY1'}, {'Collection': 'ISPY2'}, {'Collection': 'LCTSC'}, {'Collection': 'LIDC-IDRI'}, {'Collection': 'Lung Phantom'}, {'Collection': 'Lung-Fused-CT-Pathology'}, {'Collection': 'Lung-PET-CT-Dx'}, {'Collection': 'LungCT-Diagnosis'}, {'Collection': 'MIDRC-RICORD-1A'}, {'Collection': 'MIDRC-RICORD-1B'}, {'Collection': 'MIDRC-RICORD-1C'}, {'Collection': 'Mediastinal-Lymph-Node-SEG'}, {'Collection': 'Mouse-Astrocytoma'}, {'Collection': 'Mouse-Mammary'}, {'Collection': 'NSCLC Radiogenomics'}, {'Collection': 'NSCLC-Radiomics'}, {'Collection': 'NSCLC-Radiomics-Genomics'}, {'Collection': 'NSCLC-Radiomics-Interobserver1'}, {'Collection': 'NaF PROSTATE'}, {'Collection': 'PDMR-292921-168-R'}, {'Collection': 'PDMR-425362-245-T'}, {'Collection': 'PDMR-521955-158-R4'}, {'Collection': 'PDMR-833975-119-R'}, {'Collection': 'PDMR-997537-175-T'}, {'Collection': 'PDMR-BL0293-F563'}, {'Collection': 'PDMR-Texture-Analysis'}, {'Collection': 'PROSTATE-DIAGNOSIS'}, {'Collection': 'PROSTATE-MRI'}, {'Collection': 'PROSTATEx'}, {'Collection': 'Pancreas-CT'}, {'Collection': 'Pancreatic-CT-CBCT-SEG'}, {'Collection': 'Pediatric-CT-SEG'}, {'Collection': 'Pelvic-Reference-Data'}, {'Collection': 'Phantom FDA'}, {'Collection': 'Prostate Fused-MRI-Pathology'}, {'Collection': 'Prostate-3T'}, {'Collection': 'Prostate-Anatomical-Edge-Cases'}, {'Collection': 'Prostate-MRI-US-Biopsy'}, {'Collection': 'Pseudo-PHI-DICOM-Data'}, {'Collection': 'QIBA CT-1C'}, {'Collection': 'QIBA-CT-Liver-Phantom'}, {'Collection': 'QIN Breast DCE-MRI'}, {'Collection': 'QIN LUNG CT'}, {'Collection': 'QIN PET Phantom'}, {'Collection': 'QIN-BREAST'}, {'Collection': 'QIN-PROSTATE-Repeatability'}, {'Collection': 'RIDER Breast MRI'}, {'Collection': 'RIDER Lung CT'}, {'Collection': 'RIDER Lung PET-CT'}, {'Collection': 'RIDER PHANTOM MRI'}, {'Collection': 'RIDER PHANTOM PET-CT'}, {'Collection': 'RIDER Pilot'}, {'Collection': 'ReMIND'}, {'Collection': 'SPIE-AAPM Lung CT Challenge'}, {'Collection': 'Soft-tissue-Sarcoma'}, {'Collection': 'Spine-Mets-CT-SEG'}, {'Collection': 'StageII-Colorectal-CT'}, {'Collection': 'TCGA-BLCA'}, {'Collection': 'TCGA-BRCA'}, {'Collection': 'TCGA-CESC'}, {'Collection': 'TCGA-COAD'}, {'Collection': 'TCGA-ESCA'}, {'Collection': 'TCGA-KICH'}, {'Collection': 'TCGA-KIRC'}, {'Collection': 'TCGA-KIRP'}, {'Collection': 'TCGA-LIHC'}, {'Collection': 'TCGA-LUAD'}, {'Collection': 'TCGA-LUSC'}, {'Collection': 'TCGA-OV'}, {'Collection': 'TCGA-PRAD'}, {'Collection': 'TCGA-READ'}, {'Collection': 'TCGA-SARC'}, {'Collection': 'TCGA-STAD'}, {'Collection': 'TCGA-THCA'}, {'Collection': 'TCGA-UCEC'}, {'Collection': 'UPENN-GBM'}, {'Collection': 'VICTRE'}, {'Collection': 'Vestibular-Schwannoma-MC-RC'}, {'Collection': 'Vestibular-Schwannoma-SEG'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = nbia.getSeries(collection = \"Soft-tissue-Sarcoma\")\n",
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8byBMBZZail8",
        "outputId": "89c864a5-f796-4c58-e83c-30b9d3ea4b3d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'SeriesInstanceUID': '1.3.6.1.4.1.14519.5.2.1.5168.1900.104193299251798317056218297018',\n",
              " 'StudyInstanceUID': '1.3.6.1.4.1.14519.5.2.1.5168.1900.154535988064062152660648619556',\n",
              " 'Modality': 'MR',\n",
              " 'SeriesDate': '2003-12-12 00:00:00.0',\n",
              " 'SeriesDescription': '2. AXIAL T1 BOTH LEGS - RESEARCH',\n",
              " 'BodyPartExamined': 'EXTREMITY',\n",
              " 'SeriesNumber': 2,\n",
              " 'Collection': 'Soft-tissue-Sarcoma',\n",
              " 'PatientID': 'STS_010',\n",
              " 'Manufacturer': 'GE MEDICAL SYSTEMS',\n",
              " 'ManufacturerModelName': 'GENESIS_SIGNA',\n",
              " 'SoftwareVersions': '09',\n",
              " 'ImageCount': 48,\n",
              " 'TimeStamp': '2015-05-27 17:12:21.0',\n",
              " 'LicenseName': 'Creative Commons Attribution 3.0 Unported License',\n",
              " 'LicenseURI': 'http://creativecommons.org/licenses/by/3.0/',\n",
              " 'CollectionURI': 'https://doi.org/10.7937/K9/TCIA.2015.7GO2GSKS',\n",
              " 'FileSize': 25273786,\n",
              " 'DateReleased': '2015-05-27 17:12:21.0',\n",
              " 'StudyDesc': 'MRI LT LEG +C',\n",
              " 'StudyDate': '2003-12-12 00:00:00.0'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nbia.downloadSeries(data, number = 10)"
      ],
      "metadata": {
        "id": "o7gAjsGkbGCN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pydicom\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def organize_dicom_by_patient(base_dir, output_dir):\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Iterate through all the SeriesInstanceUID folders\n",
        "    for series_folder in os.listdir(base_dir):\n",
        "        series_path = os.path.join(base_dir, series_folder)\n",
        "\n",
        "        if os.path.isdir(series_path):\n",
        "            # Get all DICOM files in the folder\n",
        "            dicom_files = [f for f in os.listdir(series_path) if f.endswith(\".dcm\")]\n",
        "\n",
        "            for dicom_file in dicom_files:\n",
        "                dicom_path = os.path.join(series_path, dicom_file)\n",
        "                dicom_data = pydicom.dcmread(dicom_path)\n",
        "\n",
        "                # Extract Patient ID and Series Description from DICOM file\n",
        "                patient_id = dicom_data.PatientID\n",
        "                series_desc = dicom_data.SeriesDescription\n",
        "\n",
        "                # Create output folder based on Patient ID\n",
        "                patient_folder = os.path.join(output_dir, f\"patient_{patient_id}\")\n",
        "                os.makedirs(patient_folder, exist_ok=True)\n",
        "\n",
        "                # Copy or move the DICOM file to the patient's folder\n",
        "                shutil.copy(dicom_path, os.path.join(patient_folder, dicom_file))\n",
        "                print(f\"Copied {dicom_file} to {patient_folder}\")\n",
        "\n",
        "# Set base_dir to where your downloaded DICOM series are\n",
        "base_dir = \"/content/tciaDownload\"  # The folder containing SeriesInstanceUID subfolders\n",
        "output_dir = \"./organized_data\"\n",
        "\n",
        "organize_dicom_by_patient(base_dir, output_dir)"
      ],
      "metadata": {
        "id": "O2CWOqiXZaWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2, numpy as np\n",
        "\n",
        "def dicom_to_png(input_dir, output_dir):\n",
        "    # Ensure output directories exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Loop over patients\n",
        "    for patient_folder in os.listdir(input_dir):\n",
        "        patient_path = os.path.join(input_dir, patient_folder)\n",
        "\n",
        "        # Create output directory for the patient\n",
        "        patient_output_dir = os.path.join(output_dir, patient_folder)\n",
        "        os.makedirs(patient_output_dir, exist_ok=True)\n",
        "\n",
        "        for dicom_file in os.listdir(patient_path):\n",
        "            if dicom_file.endswith(\".dcm\"):\n",
        "                dicom_path = os.path.join(patient_path, dicom_file)\n",
        "                dicom_data = pydicom.dcmread(dicom_path)\n",
        "                pixel_array = dicom_data.pixel_array\n",
        "\n",
        "                # Normalize image to 0-255 for PNG saving\n",
        "                img = cv2.normalize(pixel_array, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "\n",
        "                # Create the PNG filename\n",
        "                png_filename = dicom_file.replace(\".dcm\", \".png\")\n",
        "\n",
        "                # Save as PNG\n",
        "                cv2.imwrite(os.path.join(patient_output_dir, png_filename), img)\n",
        "                print(f\"Saved {png_filename} in {patient_output_dir}\")\n",
        "\n",
        "# Paths to organized DICOM data and desired PNG output\n",
        "organized_dicom_dir = \"/content/organized_data\"\n",
        "png_output_dir = \"./organized_data_PNG\"\n",
        "\n",
        "dicom_to_png(organized_dicom_dir, png_output_dir)"
      ],
      "metadata": {
        "id": "YX6xwrEAeb0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from monai.networks.nets import UNet\n",
        "from torchvision.models import vit_b_16\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UNet3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UNet3D, self).__init__()\n",
        "        self.unet = UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            channels=(16, 32, 64, 128, 256),\n",
        "            strides=(2, 2, 2, 2)  # Downsampling\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.unet(x)\n",
        "\n",
        "class TransformerModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransformerModule, self).__init__()\n",
        "        self.transformer = vit_b_16(pretrained=True)  # Vision Transformer\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, depth, height, width = x.shape\n",
        "\n",
        "        # Loop through each slice in the depth dimension and resize height and width to 224x224\n",
        "        resized_slices = []\n",
        "        for i in range(depth):\n",
        "            slice_2d = x[:, :, i, :, :]  # Extract each 2D slice (N, C, H, W)\n",
        "\n",
        "            # Duplicate the grayscale channel to form 3 channels (N, 3, H, W)\n",
        "            slice_2d = slice_2d.repeat(1, 3, 1, 1)\n",
        "\n",
        "            # Resize to (224x224) as required by Vision Transformer\n",
        "            resized_slice = F.interpolate(slice_2d, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "            resized_slices.append(resized_slice)\n",
        "\n",
        "        # Stack the resized slices back into a tensor of shape (N, C, D, 224, 224)\n",
        "        resized_3d = torch.stack(resized_slices, dim=2)\n",
        "\n",
        "        # Flatten depth to treat each slice as a separate image for the Transformer\n",
        "        x = resized_3d.permute(0, 2, 1, 3, 4).reshape(batch_size * depth, 3, 224, 224)\n",
        "\n",
        "        # Apply Vision Transformer\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # Reshape the output back to original batch and depth dimensions\n",
        "        x = x.view(batch_size, depth, -1)\n",
        "        return x\n",
        "\n",
        "class Hybrid3DModel(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Hybrid3DModel, self).__init__()\n",
        "        self.unet3d = UNet3D(in_channels, out_channels)\n",
        "        self.transformer = TransformerModule()\n",
        "        self.conv_final = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 3D CNN (U-Net) for volumetric feature extraction\n",
        "        unet_output = self.unet3d(x)\n",
        "\n",
        "        # Transformer for capturing long-range dependencies\n",
        "        transformer_output = self.transformer(x)\n",
        "\n",
        "        # Combine 3D CNN output with Transformer output\n",
        "        combined = unet_output + transformer_output\n",
        "\n",
        "        # Final convolution for output prediction\n",
        "        output = self.conv_final(combined.unsqueeze(1))\n",
        "        return output\n",
        "\n",
        "# Instantiate the hybrid model\n",
        "model = Hybrid3DModel(in_channels=1, out_channels=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFlzwISsmoyW",
        "outputId": "d8fe8bd4-a35e-4b13-fdff-28fa867d04fe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|██████████| 330M/330M [00:02<00:00, 140MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_msssim import ssim\n",
        "import torch.optim as optim\n",
        "\n",
        "# Loss functions\n",
        "mse_loss = nn.MSELoss()  # Mean Squared Error for pixel-wise comparison\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4)  # AdamW optimizer"
      ],
      "metadata": {
        "id": "0mBsy7dboVfS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class MedicalImageDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.patient_dirs = sorted(os.listdir(image_dir))  # List of patient folders\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_dirs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        patient_folder = os.path.join(self.image_dir, self.patient_dirs[idx])\n",
        "        slice_files = sorted(os.listdir(patient_folder))  # List of slice images\n",
        "\n",
        "        slices = []\n",
        "        for slice_file in slice_files:\n",
        "            img_path = os.path.join(patient_folder, slice_file)\n",
        "            image = Image.open(img_path).convert(\"L\")  # Grayscale\n",
        "            image = np.array(image, dtype=np.float32)\n",
        "            slices.append(image)\n",
        "\n",
        "        # Stack slices to create a 3D volume\n",
        "        volume = np.stack(slices, axis=0)  # Shape: [depth, height, width]\n",
        "        volume = torch.FloatTensor(volume).unsqueeze(0)  # Add channel dimension\n",
        "\n",
        "        if self.transform:\n",
        "            volume = self.transform(volume)\n",
        "\n",
        "        return volume"
      ],
      "metadata": {
        "id": "w6zTnISyrXBa"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create the dataset\n",
        "train_dataset = MedicalImageDataset(image_dir='/content/organized_data_PNG/train')\n",
        "test_dataset = MedicalImageDataset(image_dir='/content/organized_data_PNG/test/')\n",
        "\n",
        "# Adjust batch size as needed (start with batch_size=1)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "lg9EUYLjfhto"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, optimizer, epochs=10):\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs in train_loader:\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            outputs = model(inputs)  # Forward pass\n",
        "\n",
        "            # Calculate the loss\n",
        "            loss_mse = mse_loss(outputs, inputs)  # Assuming reconstruction task\n",
        "            loss_ssim = 1 - ssim(outputs, inputs, data_range=outputs.max() - outputs.min())\n",
        "            total_loss = loss_mse + 0.5 * loss_ssim\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += total_loss.item()\n",
        "\n",
        "        # Print the average loss per epoch\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, optimizer, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "wSmLIXDkccZW",
        "outputId": "53f4bf56-dda3-4a55-bad7-0174762646f7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (512) must match the size of tensor b (1000) at non-singleton dimension 4",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-08a9a62eeccf>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-08a9a62eeccf>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer, epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Zero the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;31m# Calculate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-f14620a4a6ad>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# Combine 3D CNN output with Transformer output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munet_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtransformer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Final convolution for output prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (1000) at non-singleton dimension 4"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ddFT_gErclsL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}